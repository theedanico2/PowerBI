{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**LAB TASK: Categorical Feature Encoding**\n",
        "<p>Binary Classification, with every feature a categorical <p>\n",
        "\n",
        "**Problem Statement**\n",
        "<p>A common task in machine learning pipelines is encoding categorical variables for a given algorithm in a format that allows as much useful signal as possible to be captured.\n",
        "\n",
        "We have to handle different types of categorical data columns using multiple techniques in order to get best results. <p/>\n",
        "\n",
        "Lets begin.\n",
        "\n",
        "Types of categorical data given to us:\n",
        "\n",
        "* binary features\n",
        "* low- and high-cardinality nominal features\n",
        "* low- and high-cardinality ordinal features\n",
        "* (potentially) cyclical features"
      ],
      "metadata": {
        "id": "uuUvYUmta1i9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is available here https://disk.yandex.ru/d/A6zRRWGUBxytQg\n"
      ],
      "metadata": {
        "id": "89ZpUDDtqoJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Importing Libraries <h1/>"
      ],
      "metadata": {
        "id": "yod7VYG9c0ku"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8C9PCKafaxpN"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "from category_encoders import TargetEncoder, HashingEncoder, LeaveOneOutEncoder\n",
        "\n",
        "\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets load the datasets first"
      ],
      "metadata": {
        "id": "XEV826wAdyhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('')\n",
        "test = pd.read_csv('')"
      ],
      "metadata": {
        "id": "lkWDxZuYdxpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets see how the train dataset looks like"
      ],
      "metadata": {
        "id": "0ZbYkXKteD5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "srMvhBoHeDat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "zvA5d2oUeMQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.isna().sum().sort_values(ascending = False)"
      ],
      "metadata": {
        "id": "Qg_xKKEqeSXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Analyzing Categorical variables </h1>"
      ],
      "metadata": {
        "id": "Ta-6RnzheegG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of categorical variables\n",
        "s = (train.dtypes == 'object')\n",
        "object_cols = list(s[s].index)\n",
        "\n",
        "print(\"Categorical variables:\")\n",
        "print(object_cols)"
      ],
      "metadata": {
        "id": "w70lIomReknZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we observed somethings:\n",
        "\n",
        "1. There are no null values in train dataset\n",
        "2. There are multiple categorical variables which are as follows\n",
        "* bin_3, bin_4 :- binary cols\n",
        "* nom_0 - nom_4 :- nominal columns ( with no order)\n",
        "* nom_5 - nom_9 :- nominal columns with high cardinality\n",
        "* ord_1 - ord_5 :- Ordered columns\n",
        "\n",
        "We have to use different ways to treat these columns and convert them into numerical data"
      ],
      "metadata": {
        "id": "WNTeQa1ues7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Encoding techniques<h1/>"
      ],
      "metadata": {
        "id": "lgobgXgifE8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taken reference from https://www.kaggle.com/discdiver/category-encoders-examples\n",
        "\n",
        "1. bin_3, bin_4 :- Convert Y/N and T/F to 1/0\n",
        "2. nom_0 - nom_4 :- Encode using One hot encoding\n",
        "3. nom_5 - nom_9 :- Target encode them as they are high cardinal variables\n",
        "4. ord_1, ord_2 :- Convert into numerical order using hard coded values as Label encoder might not be able to understand the order\n",
        "5. ord_3 - ord_4 :- Encode using ascii as they are alphabetical values\n",
        "6. ord_5 :- Separate two alphabets and then do label encoding\n",
        "7. day, month:- Encode using sin and cosine values as they are cyclic in nature"
      ],
      "metadata": {
        "id": "Pg97IuQyfZeo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Encoding data<h1/>"
      ],
      "metadata": {
        "id": "I7gLLfMzfyRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets save target variable somewhere"
      ],
      "metadata": {
        "id": "OEjJKnQEf3kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = train['target']"
      ],
      "metadata": {
        "id": "WSvUWGOYeVao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets identify the uncommon columns between test and train data. Replace uncommon columns with a common value"
      ],
      "metadata": {
        "id": "yBsaIRASgSJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replace_xor = lambda x: 'xor' if x in xor_values else x"
      ],
      "metadata": {
        "id": "BaxfkK6XgVW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(train['ord_4'].unique()))\n",
        "print(set(test['ord_4'].unique()))"
      ],
      "metadata": {
        "id": "aIb8Qxk3gb6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_test = ['ord_5', 'ord_4', 'ord_3']\n",
        "for column in columns_to_test:\n",
        "    xor_values = set(train[column].unique()) ^ set(test[column].unique())\n",
        "    if xor_values:\n",
        "        print('Column', column, 'has', len(xor_values), 'XOR values')\n",
        "        train[column] = train[column].apply(replace_xor)\n",
        "        test[column] = test[column].apply(replace_xor)\n",
        "    else:\n",
        "        print('Column', column, 'has no XOR values')"
      ],
      "metadata": {
        "id": "qYdXSBDlgc3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def date_cyc_enc(df, col, max_vals):\n",
        "    df[col + '_sin'] = np.sin(2 * np.pi * df[col]/max_vals)\n",
        "    df[col + '_cos'] = np.cos(2 * np.pi * df[col]/max_vals)\n",
        "    return df"
      ],
      "metadata": {
        "id": "c36IqCrXgg9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_to_ascii_index = lambda x: string.ascii_letters.index(x)"
      ],
      "metadata": {
        "id": "czRVixOvgqKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replace_xor = lambda x: 'xor' if x in xor_values else x"
      ],
      "metadata": {
        "id": "5aQmjSZUgtGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting with encoding lets create some other features"
      ],
      "metadata": {
        "id": "2e0h193OgybQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['merge_col1'] =  train[['nom_0', 'nom_1']].apply(lambda x: ''.join(x), axis=1)\n",
        "test['merge_col1'] =  test[['nom_0', 'nom_1']].apply(lambda x: ''.join(x), axis=1)\n",
        "\n",
        "train['merge_col2'] =  train[['nom_1', 'nom_2']].apply(lambda x: ''.join(x), axis=1)\n",
        "test['merge_col2'] =  test[['nom_1', 'nom_3']].apply(lambda x: ''.join(x), axis=1)\n",
        "\n",
        "train['merge_col3'] =  train[['nom_2', 'nom_3']].apply(lambda x: ''.join(x), axis=1)\n",
        "test['merge_col3'] =  test[['nom_2', 'nom_3']].apply(lambda x: ''.join(x), axis=1)\n",
        "\n",
        "train['merge_col4'] =  train[['nom_3', 'nom_4']].apply(lambda x: ''.join(x), axis=1)\n",
        "test['merge_col4'] =  test[['nom_3', 'nom_4']].apply(lambda x: ''.join(x), axis=1)"
      ],
      "metadata": {
        "id": "GNGpavVkg2aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding"
      ],
      "metadata": {
        "id": "otHJbOgyg6Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary encoding\n",
        "train['bin_3'] = [0 if x == 'F' else 1 for x in train['bin_3']]\n",
        "train['bin_4'] = [0 if x == 'N' else 1 for x in train['bin_4']]\n",
        "\n",
        "#Hard coded Label encoding\n",
        "train['ord_1'] = [0 if x == 'Novice' else 1 if x == 'Contributor' else 2 if x == 'Expert' else 3 if x == 'Master' else 4 for x in train['ord_1']]\n",
        "train['ord_2'] = [0 if x == 'Freezing' else 1 if x == 'Cold' else 2 if x == 'Warm' else 3 if x == 'Hot' else 4 if x == 'Boiling Hot' else 5 for x in train['ord_2']]\n",
        "\n",
        "# Label encoding via LabelEncoder class\n",
        "label_encoder = LabelEncoder()\n",
        "train['ord_3'] = label_encoder.fit_transform(train['ord_3'])\n",
        "test['ord_3'] = label_encoder.transform(test['ord_3'])\n",
        "\n",
        "train['ord_4'] = label_encoder.fit_transform(train['ord_4'])\n",
        "test['ord_4'] = label_encoder.transform(test['ord_4'])\n",
        "\n",
        "train['ord_5'] = label_encoder.fit_transform(train['ord_5'])\n",
        "test['ord_5'] = label_encoder.transform(test['ord_5'])\n",
        "\n",
        "train = date_cyc_enc(train, 'day', 7)\n",
        "train = date_cyc_enc(train, 'month', 12)\n",
        "train.drop(['day', 'month'], axis=1, inplace = True)\n",
        "\n",
        "#Leave one out encoding high cardinal variables\n",
        "high_cardinal_vars = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\n",
        "\n",
        "loo_encoder = LeaveOneOutEncoder(cols=high_cardinal_vars)\n",
        "train = loo_encoder.fit_transform(train.drop(['target'], axis = 1), train['target'])\n",
        "\n",
        "# Same for test data\n",
        "test['bin_3'] = [0 if x == 'F' else 1 for x in test['bin_3']]\n",
        "test['bin_4'] = [0 if x == 'N' else 1 for x in test['bin_4']]\n",
        "test['ord_1'] = [0 if x == 'Novice' else 1 if x == 'Contributor' else 2 if x == 'Expert' else 3 if x == 'Master' else 4 for x in test['ord_1']]\n",
        "test['ord_2'] = [0 if x == 'Freezing' else 1 if x == 'Cold' else 2 if x == 'Warm' else 3 if x == 'Hot' else 4 if x == 'Boiling Hot' else 5 for x in test['ord_2']]\n",
        "\n",
        "#For cyclic data we convert it into sin and cosine values\n",
        "test = date_cyc_enc(test, 'day', 7)\n",
        "test = date_cyc_enc(test, 'month', 12)\n",
        "test.drop(['day', 'month'], axis=1, inplace = True)\n",
        "\n",
        "test = loo_encoder.transform(test)"
      ],
      "metadata": {
        "id": "GedFwMZug-Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One Hot encoding other nominal columns\n",
        "train_df = pd.get_dummies(train, drop_first=True)\n",
        "\n",
        "# Same for test data\n",
        "test_df = pd.get_dummies(test, drop_first=True)"
      ],
      "metadata": {
        "id": "LSF-E8qghept"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.shape)\n",
        "print(test_df.shape)"
      ],
      "metadata": {
        "id": "sjKDT2NThgs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cor = train_df.corr()"
      ],
      "metadata": {
        "id": "c74AXfBqhlce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(figsize=(25, 25))\n",
        "sns.heatmap(cor, annot=False, ax=ax)"
      ],
      "metadata": {
        "id": "hB1r4tOhhrYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can clearly see that correlation between data points is quite less here. So lets keep all these features and move ahead with our classification"
      ],
      "metadata": {
        "id": "fz_jEJWshtyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_df.drop(['id'], axis=1)\n",
        "y = target"
      ],
      "metadata": {
        "id": "2yclss_yhyE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Analysing imbalance in dataset<h1/>"
      ],
      "metadata": {
        "id": "A-cPos72h4Th"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read here regarding SMOTE https://habr.com/ru/companies/otus/articles/769242/\n"
      ],
      "metadata": {
        "id": "-8owNIVAsMsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=y.value_counts()\n",
        "plt.bar(x.index,x)\n",
        "plt.gca().set_xticks([0,1])\n",
        "plt.title('distribution of target variable')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5EWOXY23h00U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly there is imbalance in dataset. We need to cater this implance using SMOTE technique"
      ],
      "metadata": {
        "id": "i30kQvOYiL3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets do the necessary train_test_split\n",
        "\n"
      ],
      "metadata": {
        "id": "6oAn778uiP5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=1)"
      ],
      "metadata": {
        "id": "wKAxUcgxiNkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets cater the imbabalnce in dataset"
      ],
      "metadata": {
        "id": "bbdMR0VNiU7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sm = SMOTE(kind = \"regular\")\n",
        "X_tr,y_tr = sm.fit_sample(X_train,y_train)"
      ],
      "metadata": {
        "id": "5lPhGB8HiTa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets create a function to test our dataset and calculate ROC-AUC score for multiple models as param\n",
        "\n"
      ],
      "metadata": {
        "id": "ORZXwMFBicqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_roc( actual, probs ):\n",
        "    fpr, tpr, thresholds = roc_curve( actual, probs,\n",
        "                                              drop_intermediate = False )\n",
        "    auc_score = roc_auc_score( actual, probs )\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic example')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "lcREszc3ipMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for comparing different approaches\n",
        "def score_dataset(X_train, X_test, y_train, y_test, model):\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    draw_roc(y_test, preds)\n",
        "    return roc_auc_score(y_test, preds)"
      ],
      "metadata": {
        "id": "2ap27xGwixls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Scaling data <h1>"
      ],
      "metadata": {
        "id": "0VtNIRoci7fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_tr = scaler.fit_transform(X_tr)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "NUk7p5qni2cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Testing with vanilla version of models <h1/>"
      ],
      "metadata": {
        "id": "dlvFq2LijGVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read here regarding ML metrics https://habr.com/ru/companies/ods/articles/328372/"
      ],
      "metadata": {
        "id": "tWj7mPyesoQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets plot roc curver and calculate score for all models"
      ],
      "metadata": {
        "id": "dz-v6hfVjLSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a simple logistic regression model (lr) with random_state 0, and solver 'lbfgs'\n",
        "\n",
        "#Print AUC score with Logistic Regression"
      ],
      "metadata": {
        "id": "ZF6SSw3tjF5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a simple decision tree classifier (dt) with random_state 0.\n",
        "\n",
        "#Print AUC score with Decision Tree"
      ],
      "metadata": {
        "id": "PWZ0ERjflIym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Model Refinement <h1/>"
      ],
      "metadata": {
        "id": "gr9jSAsIlckV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# specify number of folds for k-fold CV\n",
        "n_folds = 5\n",
        "\n",
        "# parameters to build the model on\n",
        "param_grid = {\n",
        "    'max_depth': range(1, 5),\n",
        "    'min_samples_leaf': range(25, 175, 50),\n",
        "    'min_samples_split': range(50, 150, 50)"
      ],
      "metadata": {
        "id": "jipCvW_clhpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree"
      ],
      "metadata": {
        "id": "h_h_cfqWlo3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment if you want to see hyper parameter tuning. Although it takes some good amount of time\n",
        "'''\n",
        "# instantiate the model\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "# fit tree on training data\n",
        "grid_search_dt = GridSearchCV(estimator = dt, param_grid = param_grid,\n",
        "                          cv = n_folds, verbose = 1, n_jobs = -1, scoring=\"roc_auc\")\n",
        "grid_search_dt.fit(X_tr, y_tr)\n",
        "'''"
      ],
      "metadata": {
        "id": "-9SeNrVBlnAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment to see the results\n",
        "'''\n",
        "cv_results_dt = pd.DataFrame(grid_search_dt.cv_results_)\n",
        "# printing the optimal accuracy score and hyperparameters\n",
        "print(\"Decison Tree grid search Accuracy : \", grid_search_dt.best_score_)\n",
        "print(grid_search_dt.best_estimator_)\n",
        "'''"
      ],
      "metadata": {
        "id": "lmog6bo-l6rC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "AkUI4Y-fl-5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logit_param_grid = {\n",
        "    'C': [0.100, 0.150, 0.120, 0.125, 0.130, 0.135, 0.140, 0.145, 0.150]\n",
        "}\n",
        "\n",
        "logit_grid = GridSearchCV(estimator = lr, param_grid = logit_param_grid,\n",
        "                          scoring='roc_auc', cv=5, n_jobs=-1, verbose=0)\n",
        "logit_grid.fit(X_tr, y_tr)\n",
        "\n",
        "best_C = logit_grid.best_params_['C']\n",
        "# best_C = C = 0.125\n",
        "\n",
        "print('Best C:', best_C)"
      ],
      "metadata": {
        "id": "e4SH_Zzgl8Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model fitting with tuned hyper parameters"
      ],
      "metadata": {
        "id": "IkJy1E_SmJD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
        "                       max_features=None, max_leaf_nodes=None,\n",
        "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                       min_samples_leaf=25, min_samples_split=50,\n",
        "                       min_weight_fraction_leaf=0.0, presort=False,\n",
        "                       random_state=None, splitter='best')\n",
        "print('AUC score with Decision Tree :- ', score_dataset(X_tr, X_test, y_tr, y_test, dt))"
      ],
      "metadata": {
        "id": "ded3XTWFmM1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(solver='lbfgs', random_state = 0, C=best_C)\n",
        "print('AUC score with Losgistic Regression :- ', score_dataset(X_tr, X_test, y_tr, y_test, lr))"
      ],
      "metadata": {
        "id": "ZSTjdJrjmi0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets predict for test data"
      ],
      "metadata": {
        "id": "GahljGpTmt-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.drop(['id'], axis=1, inplace = True)"
      ],
      "metadata": {
        "id": "Cc5R24immnfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets scale test data first"
      ],
      "metadata": {
        "id": "MQssKyakmzjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Scale (transform only) test_df"
      ],
      "metadata": {
        "id": "YWdH8jbFmy2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets fit for entire training set before making predictions. But before that we have to scale entire train data also"
      ],
      "metadata": {
        "id": "CZAkTffInMdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Do the same to X"
      ],
      "metadata": {
        "id": "6DyDLi8unLZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = lr.fit(X,y)\n",
        "dt = dt.fit(X,y)"
      ],
      "metadata": {
        "id": "Nb7CNTIpnY08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_final_lr = lr.predict(test_df)\n",
        "y_test_final_dt = dt.predict(test_df)"
      ],
      "metadata": {
        "id": "Eq-ZHBfJnb0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_prob_lr = lr.predict_proba(test_df)[:, 1]\n",
        "y_test_prob_dt = dt.predict_proba(test_df)[:, 1]"
      ],
      "metadata": {
        "id": "zOmzdJUZnhTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_lr = pd.DataFrame({\n",
        "        \"id\": test[\"id\"],\n",
        "        \"target\": y_test_prob_lr\n",
        "    })"
      ],
      "metadata": {
        "id": "TofQPtiKnt1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Do the same for submission_dt"
      ],
      "metadata": {
        "id": "t8J-cNicnymQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}